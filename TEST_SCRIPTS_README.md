# Test Scripts for KangurooAvatar

This directory contains test scripts to help developers understand how the AI services work and how viseme data is generated.

## ðŸ§ª Test Scripts

### `test_services.py` - Complete Service Integration Test
**Purpose**: Demonstrates the full pipeline from user input to viseme data generation

**What it does**:
- Tests Gemini service with sample questions
- Tests ElevenLabs TTS with viseme generation
- Shows complete pipeline: Gemini â†’ ElevenLabs â†’ Viseme Data
- Displays performance metrics and timing
- Shows viseme statistics and breakdown

**Usage**:
```bash
python test_services.py
```

**Output**:
- Generated audio files (test_audio_*.mp3)
- Detailed viseme data structure
- Performance timing for each step
- Viseme frequency analysis

### `test_viseme_data.py` - Viseme Data Structure Test
**Purpose**: Focuses specifically on viseme data generation and structure

**What it does**:
- Tests multiple text samples
- Shows exact JSON structure of viseme data
- Provides viseme reference guide
- Shows frontend implementation examples
- Demonstrates viseme timing and intensity

**Usage**:
```bash
python test_viseme_data.py
```

**Output**:
- Raw JSON viseme data
- Viseme timing breakdown
- Implementation code examples
- Viseme reference guide

## ðŸ”§ Prerequisites

Before running the test scripts, ensure you have:

1. **API Keys Configured**:
   ```bash
   # Copy environment template
   cp env_example.txt .env
   
   # Edit .env with your API keys
   GEMINI_API_KEY=your_actual_gemini_key
   ELEVENLABS_API_KEY=your_actual_elevenlabs_key
   ELEVENLABS_VOICE_ID=your_voice_id
   ```

2. **Dependencies Installed**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Project Files Present**:
   - `faq_data.json` (for Gemini context)
   - `config.py` (for API configuration)
   - Service modules (`gemini_service.py`, `elevenlabs_service.py`)

## ðŸ“Š Understanding the Output

### Viseme Data Structure
The test scripts show the exact structure of viseme data:

```json
{
  "visemes": [
    {"time": 0.0, "viseme": "viseme_aa"},
    {"time": 0.2, "viseme": "viseme_ee"},
    {"time": 0.4, "viseme": "viseme_ii"}
  ],
  "duration": 2.5,
  "text": "Hello, how are you?"
}
```

### Viseme Types
- `viseme_sil`: Silence (closed mouth)
- `viseme_aa`: A sound (open mouth)
- `viseme_ee`: E sound (smile shape)
- `viseme_ii`: I sound (narrow smile)
- `viseme_oo`: O sound (rounded lips)
- `viseme_uu`: U sound (puckered lips)
- `viseme_kk`: K sound (closed mouth)
- `viseme_PP`: P sound (lips together)
- `viseme_ff`: F sound (lip touch)
- `viseme_DD`: D sound (tongue touch)
- `viseme_ss`: S sound (narrow opening)

### Performance Metrics
- **Gemini Processing**: Target <2000ms
- **ElevenLabs TTS**: Target <1500ms
- **Total Pipeline**: Target <4000ms

## ðŸŽ¯ What Developers Learn

### From `test_services.py`:
1. How Gemini generates context-aware responses
2. How ElevenLabs converts text to speech with visemes
3. Complete pipeline timing and performance
4. Error handling and fallback mechanisms
5. Integration between services

### From `test_viseme_data.py`:
1. Exact structure of viseme data
2. Timing information for lip-sync
3. Viseme intensity mapping
4. Frontend implementation examples
5. How to apply visemes to 3D characters

## ðŸ”§ Frontend Integration

The viseme data generated by these tests is used in the frontend like this:

```javascript
// Start lip-sync with viseme data
function startLipSync(visemeData) {
    const visemes = visemeData.visemes;
    const startTime = Date.now();
    
    function animateViseme() {
        const currentTime = (Date.now() - startTime) / 1000;
        
        // Find current viseme
        let currentViseme = null;
        for (const viseme of visemes) {
            if (viseme.time <= currentTime) {
                currentViseme = viseme;
            }
        }
        
        // Apply to 3D character
        if (currentViseme) {
            applyVisemeToCharacter(currentViseme.viseme);
        }
        
        // Continue if not finished
        if (currentTime < visemeData.duration) {
            requestAnimationFrame(animateViseme);
        }
    }
    
    animateViseme();
}
```

## ðŸš€ Running the Tests

1. **Quick Test** (viseme data only):
   ```bash
   python test_viseme_data.py
   ```

2. **Full Pipeline Test** (complete system):
   ```bash
   python test_services.py
   ```

3. **Check Output**:
   - Look for generated audio files
   - Review console output for viseme data
   - Check performance metrics

## ðŸ“ Generated Files

The test scripts generate:
- `test_audio_*.mp3` - Audio files from ElevenLabs
- `viseme_test_*.mp3` - Audio files from viseme tests
- Console output with detailed viseme data
- Performance metrics and timing

## ðŸ” Troubleshooting

### Common Issues:
1. **API Key Errors**: Ensure your .env file has correct API keys
2. **Import Errors**: Run from the project root directory
3. **Network Errors**: Check internet connection for API calls
4. **File Errors**: Ensure all project files are present

### Debug Mode:
Add debug prints to see detailed information:
```python
# In the test scripts, you can add:
print(f"Debug: Viseme data = {viseme_data}")
print(f"Debug: Audio file = {audio_file}")
```

## ðŸ’¡ Tips for Developers

1. **Start with `test_viseme_data.py`** to understand the data structure
2. **Use `test_services.py`** to see the complete pipeline
3. **Check the console output** for detailed viseme information
4. **Listen to generated audio** to understand the timing
5. **Use the JSON structure** to implement frontend lip-sync

These test scripts provide everything developers need to understand and implement the viseme-based lip-sync system in their own applications.
